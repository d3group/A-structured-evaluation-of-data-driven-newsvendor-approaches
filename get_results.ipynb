{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e861b30-3c9c-4ca6-b198-44ebd4e21bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n",
      "<ipython-input-103-0dffcba2125a>:233: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  best_results = best_results.append(d, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from ddop.metrics import average_costs, prescriptiveness_score\n",
    "from ddop.newsvendor import SampleAverageApproximationNewsvendor\n",
    "from ddop.newsvendor import DecisionTreeWeightedNewsvendor\n",
    "from ddop.newsvendor import RandomForestWeightedNewsvendor \n",
    "from ddop.newsvendor import KNeighborsWeightedNewsvendor\n",
    "from ddop.newsvendor import LinearRegressionNewsvendor\n",
    "from ddop.newsvendor import GaussianWeightedNewsvendor\n",
    "from ddop.newsvendor import LinearRegressionNewsvendor\n",
    "from ddop.newsvendor import DeepLearningNewsvendor\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#####################################################################################\n",
    "###                                    SETTINGS                                    ##\n",
    "#####################################################################################\n",
    "\n",
    "# define jobs for multiprocessing\n",
    "n_jobs = 1\n",
    "\n",
    "# define cv strategy\n",
    "n_splits = 10\n",
    "cv = KFold(n_splits=n_splits)\n",
    "\n",
    "# define estimator tuples test\n",
    "estimator_tuple_list = []\n",
    "estimator_tuple_list.append(('SAA', SampleAverageApproximationNewsvendor()))\n",
    "estimator_tuple_list.append(('LR', LinearRegressionNewsvendor()))\n",
    "#estimator_tuple_list.append(('DTW', DecisionTreeWeightedNewsvendor(random_state=1)))\n",
    "#estimator_tuple_list.append(('RFW', RandomForestWeightedNewsvendor(random_state=1)))\n",
    "#estimator_tuple_list.append(('KNNW',KNeighborsWeightedNewsvendor()))\n",
    "#estimator_tuple_list.append(('GKW', GaussianWeightedNewsvendor()))\n",
    "#estimator_tuple_list.append(('DL', DeepLearningNewsvendor(random_state=1)))\n",
    "\n",
    "# define feature categories \n",
    "feature_cat_dict = {\n",
    "    \"calendar\": ['weekday', 'month', 'year'],\n",
    "    \"lag\": ['demand__sum_values_7', 'demand__median_7',\n",
    "       'demand__mean_7', 'demand__standard_deviation_7', 'demand__variance_7',\n",
    "       'demand__root_mean_square_7', 'demand__maximum_7',\n",
    "       'demand__absolute_maximum_7', 'demand__minimum_7',\n",
    "       'demand__sum_values_14', 'demand__median_14', 'demand__mean_14',\n",
    "       'demand__standard_deviation_14', 'demand__variance_14',\n",
    "       'demand__root_mean_square_14', 'demand__maximum_14',\n",
    "       'demand__absolute_maximum_14', 'demand__minimum_14',\n",
    "       'demand__sum_values_28', 'demand__median_28', 'demand__mean_28',\n",
    "       'demand__standard_deviation_28', 'demand__variance_28',\n",
    "       'demand__root_mean_square_28', 'demand__maximum_28',\n",
    "       'demand__absolute_maximum_28', 'demand__minimum_28'],\n",
    "    \"weather_yaz\": ['wind', 'clouds', 'rain', 'sunshine', 'temperature'],\n",
    "    \"special_m5\": ['is_sporting_event', 'is_cultural_event', 'is_national_event', 'is_religious_event', 'is_snap_day']}\n",
    "\n",
    "# define all datasets to run with the corresponding feature categories\n",
    "dataset_dict = {\n",
    "    \"m5\": [[\"calendar\"], [\"calendar\", \"lag\"],[\"calendar\", \"lag\", \"special_m5\"]],\n",
    "    \"SID\": [[\"calendar\"], [\"calendar\", \"lag\"]],\n",
    "    \"YAZ\": [[\"calendar\"], [\"calendar\", \"lag\"], [\"calendar\", \"lag\", \"weather_yaz\"]]\n",
    "}\n",
    "\n",
    "# define under- and overage costs\n",
    "cu = [9, 7.5, 5, 2.5, 1]\n",
    "co = [1, 2.5, 5, 7.5, 9]\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# define grids\n",
    "def get_grid(estimator_name, n_features):\n",
    "    if estimator_name == \"DTW\":\n",
    "        grid = {\n",
    "            \"max_depth\":[None,2,4,6,8,10],\n",
    "            \"min_samples_split\": [2,4,6,8,16,32,64]\n",
    "        }\n",
    "        \n",
    "    elif estimator_name == \"RFW\":\n",
    "        grid = {\n",
    "            \"max_depth\":[None,2,4,6,8,10],\n",
    "            'min_samples_split':[2,4,6,8,16,32,64],\n",
    "            'n_estimators':[10,20,50,100]}\n",
    "        \n",
    "    elif estimator_name == \"KNNW\":\n",
    "        grid = {'n_neighbors':[1,2,4,8,16,32,64,128]}\n",
    "        \n",
    "    elif estimator_name == \"GKW\":\n",
    "        grid = {'kernel_bandwidth':[*np.arange(0.5, int(np.sqrt(n_features/2))+0.25, 0.25)]}\n",
    "        \n",
    "    elif estimator_name == \"DL\":\n",
    "        grid = {\"optimizer\": [\"adam\"],\n",
    "                \"neurons\": [\n",
    "                    (round(0.5*n_features),round(0.5*0.5*n_features)),\n",
    "                    (round(0.5*n_features),round(0.5*1*n_features)),\n",
    "                    (1*n_features,round(1*0.5*n_features)),\n",
    "                    (1*n_features,1*1*n_features),\n",
    "                    (2*n_features,round(2*0.5*n_features)),\n",
    "                    (2*n_features,2*1*n_features),\n",
    "                    (3*n_features,round(3*0.5*n_features)),\n",
    "                    (3*n_features,3*1*n_features)],\n",
    "                \"epochs\": [10,100,200]}\n",
    "    else:\n",
    "        grid = None\n",
    "        \n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_wsaa_sl_scores(X, y, params, cu, co, estimator, cv):\n",
    "    estimator.set_params(**params)\n",
    "    scores = []\n",
    "    sl_scores = []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        estimator.fit(X[train_index],y[train_index])\n",
    "        for cu_i, co_i in zip(cu,co):\n",
    "            estimator.cu = [cu_i]\n",
    "            estimator.co = [co_i]\n",
    "            estimator.cu_ = [cu_i]\n",
    "            estimator.co_ = [co_i]\n",
    "            score = estimator.score(X[test_index],y[test_index])\n",
    "            scores.append(score)\n",
    "        sl_scores.append(scores)\n",
    "        scores = []\n",
    "\n",
    "    return sl_scores\n",
    "\n",
    "\n",
    "def get_wsaa_results(group, X_train, X_test, y_train, y_test, param_grid, cu, co, estimator, estimator_name, cv, dataset, feature_combi, scaler_target, n_jobs):\n",
    "    \n",
    "    cv_results = pd.DataFrame()\n",
    "    best_results = pd.DataFrame()\n",
    "\n",
    "    candidate_params = list(ParameterGrid(param_grid))\n",
    "        \n",
    "    parallel = Parallel(n_jobs=n_jobs)\n",
    "    scores = parallel(delayed(get_wsaa_sl_scores)(X_train, y_train, params, cu, co, estimator, cv) for params in candidate_params)\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    mean_scores = scores.mean(axis=1)\n",
    "    rank_scores = mean_scores.argmax(axis=0)\n",
    "    best_scores = mean_scores.max(axis=0)\n",
    "    best_params = [candidate_params[rank] for rank in rank_scores]\n",
    "\n",
    "    for i in range(len(cu)):\n",
    "        cv_results_temp = pd.DataFrame(scores.T[i].T, columns = ['split'+str(split)+'_test_score' for split in range(n_splits)])\n",
    "        cv_results_temp[\"mean_test_score\"] = mean_scores.T[i]\n",
    "        cv_results_temp[\"dataset\"] = dataset\n",
    "        cv_results_temp[\"feature combi\"] = str(feature_combi)\n",
    "        cv_results_temp[\"group\"] = str(group)\n",
    "        cv_results_temp[\"model\"] = estimator_name\n",
    "        cv_results_temp[\"cu\"] = cu[i]\n",
    "        cv_results_temp[\"co\"] = co[i]\n",
    "        cv_results_temp[\"sl\"] = cu[i]/(cu[i]+co[i])\n",
    "        cv_results_temp[\"params\"] = candidate_params\n",
    "        cv_results = pd.concat([cv_results, cv_results_temp], ignore_index=True)\n",
    "\n",
    "    for i in range(len(cu)):\n",
    "        best_estimator = clone(estimator).set_params(**best_params[i])\n",
    "        best_estimator.set_params(cu=cu[i],co=co[i])\n",
    "        best_estimator.fit(X_train, y_train)\n",
    "        pred = best_estimator.predict(X_test)\n",
    "        pred = scaler_target.inverse_transform(pred)\n",
    "        avg_costs = average_costs(y_test,pred,cu[i],co[i])\n",
    "        avg_costs = round(avg_costs,4)\n",
    "        saa_pred = SampleAverageApproximationNewsvendor(cu[i],co[i]).fit(y_train).predict(X_test.shape[0])\n",
    "        saa_pred = scaler_target.inverse_transform(saa_pred)\n",
    "        SoP = prescriptiveness_score(y_test, pred, saa_pred, cu[i], co[i])\n",
    "        SoP = round(SoP,4)\n",
    "\n",
    "        d = {'dataset': dataset, 'feature combi': feature_combi, 'group': str(group), 'model': estimator_name, 'cu': cu[i], 'co': co[i], 'sl': cu[i]/(cu[i]+co[i]), 'average costs': avg_costs, 'coefficient of prescriptiveness': SoP, 'best params': best_params[i]}\n",
    "        best_results = best_results.append(d, ignore_index=True)\n",
    "    \n",
    "    return cv_results, best_results\n",
    "\n",
    "\n",
    "def get_model_results(group, X_train, X_test, y_train, y_test, param_grid, cu, co, estimator, estimator_name, cv, dataset, feature_combi, scaler_target):\n",
    "    \n",
    "    cv_results = pd.DataFrame()\n",
    "    best_results = pd.DataFrame()\n",
    "    \n",
    "    for cu_i, co_i in zip(cu,co):\n",
    "        \n",
    "        base_estimator = clone(estimator)\n",
    "        base_estimator.set_params(cu=cu_i,co=co_i)\n",
    "        \n",
    "        if estimator_name in [\"LR\", \"SAA\"]:\n",
    "            \n",
    "            #cv_scores = cross_val_score(estimator, y_train, cv=cv)\n",
    "            cv_scores = cross_val_score(base_estimator, X=X_train, y=y_train, cv=cv)\n",
    "            cv_results_temp = pd.DataFrame([cv_scores], columns = ['split'+str(split)+'_test_score' for split in range(10)])\n",
    "            cv_results_temp[\"mean_test_score\"] = cv_scores.mean()\n",
    "            best_estimator = base_estimator\n",
    "            best_estimator.fit(X=X_train, y=y_train)\n",
    "            params = np.nan\n",
    "            best_params = np.nan\n",
    "        \n",
    "        else:\n",
    "            gs = GridSearchCV(base_estimator, param_grid, cv=cv)\n",
    "            gs.fit(X_train,y_train)\n",
    "            cv_results_temp = pd.DataFrame({k: v for k, v in gs.cv_results_.items() if k.startswith('split') or k == 'mean_test_score'})\n",
    "            best_estimator = gs.best_estimator_\n",
    "            params = gs.cv_results_[\"params\"]\n",
    "            best_params = gs.best_params_\n",
    "            \n",
    "            \n",
    "        cv_results_temp[\"dataset\"] = dataset\n",
    "        cv_results_temp[\"feature combi\"] = str(feature_combi)\n",
    "        cv_results_temp[\"group\"] = str(group)\n",
    "        cv_results_temp[\"model\"] = estimator_name\n",
    "        cv_results_temp[\"cu\"] = cu_i\n",
    "        cv_results_temp[\"co\"] = co_i\n",
    "        cv_results_temp[\"sl\"] = cu_i/(cu_i+co_i)\n",
    "        cv_results_temp[\"params\"] = params\n",
    "        cv_results = pd.concat([cv_results, cv_results_temp], ignore_index=True)\n",
    "\n",
    "        if estimator_name == \"SAA\":\n",
    "            pred = best_estimator.predict(X_test.shape[0])\n",
    "            \n",
    "        else:\n",
    "            pred = best_estimator.predict(X_test)\n",
    "            \n",
    "        pred = scaler_target.inverse_transform(pred)\n",
    "        avg_costs = average_costs(y_test,pred,cu_i,co_i)\n",
    "        avg_costs = round(avg_costs,4)        \n",
    "\n",
    "        saa_pred = SampleAverageApproximationNewsvendor(cu_i,co_i).fit(y_train).predict(X_test.shape[0])\n",
    "        saa_pred = scaler_target.inverse_transform(saa_pred)\n",
    "        SoP = prescriptiveness_score(y_test, pred, saa_pred, cu_i, co_i)\n",
    "        SoP = round(SoP,4)\n",
    "\n",
    "        d = {'dataset': dataset, 'feature combi': feature_combi, 'group': str(group), 'model': estimator_name, 'cu': cu_i, 'co': co_i, 'sl': cu_i/(cu_i+co_i), 'average costs': avg_costs, 'coefficient of prescriptiveness': SoP, 'best params': best_params}\n",
    "        best_results = best_results.append(d, ignore_index=True)\n",
    "        \n",
    "    return cv_results, best_results\n",
    "\n",
    "\n",
    "def get_results(group, X, y, cu, co, estimator_tuple_list, cv, dataset, feature_combi):\n",
    "    \n",
    "    cv_results = pd.DataFrame()\n",
    "    best_results = pd.DataFrame()\n",
    "    \n",
    "    X = X.get_group(group)\n",
    "    y = y.iloc[X.index.values.tolist()]\n",
    "\n",
    "    X = X.drop([\"store\", \"item\"], axis=1)\n",
    "    n_features = len(X.columns)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=False)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    #scale target variable\n",
    "    scaler_target = StandardScaler()\n",
    "    scaler_target.fit(y_train)\n",
    "    y_train = scaler_target.transform(y_train).ravel()\n",
    "    \n",
    "    for estimator_tuple in estimator_tuple_list:\n",
    "        \n",
    "        estimator_name = estimator_tuple[0]\n",
    "        estimator = estimator_tuple[1]\n",
    "        param_grid = get_grid(estimator_name, n_features)\n",
    "        \n",
    "        if estimator_name in [\"KNNW\", \"RFW\", \"DTW\", \"GKW\"]:\n",
    "            cv_results_temp, best_results_temp = get_wsaa_results(group, X_train, X_test, y_train, y_test, param_grid, cu, co, estimator, estimator_name, cv, dataset, feature_combi, scaler_target, n_jobs=1)\n",
    "        \n",
    "        else:\n",
    "            cv_results_temp, best_results_temp = get_model_results(group, X_train, X_test, y_train, y_test, param_grid, cu, co, estimator, estimator_name, cv, dataset, feature_combi, scaler_target)\n",
    "        \n",
    "        cv_results = pd.concat([cv_results, cv_results_temp], ignore_index=True)\n",
    "        best_results = pd.concat([best_results, best_results_temp], ignore_index=True)\n",
    "        \n",
    "    return cv_results, best_results\n",
    "\n",
    "\n",
    "for dataset in dataset_dict:\n",
    "    \n",
    "    X = pd.read_csv(\"Data/final/\"+dataset+\"_data.csv.zip\")\n",
    "    y = pd.read_csv(\"Data/final/\"+dataset+\"_target.csv.zip\")\n",
    "    \n",
    "    cv_results = pd.DataFrame()\n",
    "    best_results = pd.DataFrame()\n",
    "    \n",
    "    for feature_combi in dataset_dict[dataset]:\n",
    "        cols = []\n",
    "        for feature_cat in feature_combi:\n",
    "            cols = cols + feature_cat_dict[feature_cat]\n",
    "\n",
    "            X_cols = X[cols+[\"store\", \"item\"]]\n",
    "            \n",
    "        X_cols = pd.get_dummies(X_cols, columns=[\"weekday\", \"month\"])\n",
    "\n",
    "        X_grouped = X_cols.groupby([\"store\", \"item\"])\n",
    "        groups = list(X_grouped.groups.keys())\n",
    "\n",
    "        parallel = Parallel(n_jobs=n_jobs)\n",
    "        results = parallel(delayed(get_results)(group, X_grouped, y, cu, co, estimator_tuple_list, cv, dataset, feature_combi) for group in groups)\n",
    "\n",
    "        for result in results:\n",
    "            cv_results = pd.concat([cv_results, result[0]], ignore_index=True) \n",
    "            best_results = pd.concat([best_results, result[1]], ignore_index=True)\n",
    "            \n",
    "        cv_results.to_csv(\"Results/cv_results.csv\", index=False)\n",
    "        best_results.to_csv(\"Results/best_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddop",
   "language": "python",
   "name": "ddop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
